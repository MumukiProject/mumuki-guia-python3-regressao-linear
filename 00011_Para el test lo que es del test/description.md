ğŸ”® Al trabajar con modelos predictivos, no solo es importante ajustarlos, sino tambiÃ©n cuantificar su capacidad para predecir nuevas observaciones.

Para poder hacer esta evaluaciÃ³n, vamos a volver a entrenarlo, pero aplicando una metodologÃ­a  muy frecuente en el campo del aprendizaje automÃ¡tico: separar los datos en dos conjuntos de datos, uno de entrenamiento (`train`) y otro de prueba (`test`). Esto lo resolveremos con la ayuda de la funciÃ³n `train_test_split` de `scikit-learn`, que particionarÃ¡ nuestras `X` e `y`, cada una, en dos conjuntos: 

```python
X = diabetes[['body_mass_index']]
y = diabetes['response']

X_train, X_test, y_train, y_test = train_test_split(
  X.values.reshape(-1,1),
  y.values,
  train_size = 0.8,
  random_state = 42,  
  shuffle = True
)
```                                    

Luego, usaremos el par `(X_train, y_train)` para ajustar los coeficientes de nuestro regresor...

```python
modelo = LinearRegression()
modelo.fit(X = X_train, y = y_train)
print("Ordenada:", modelo.intercept_)
print("Pendiente:", list(zip(X.columns, modelo.coef_.flatten())))
```

...pero ahora evaluaremos la capacidad predictiva usando el par `(X_test, y_test)`:

```python
print("Coeficiente de determinaciÃ³n RÂ²:", modelo.score(X_test, y_test))
```

De esta forma, evitaremos _hacer trampa_ y evaluar al modelo usando los propios datos con lo que fue entrenado ğŸ˜‰.

> Veamos si se va entendiendo: entrenÃ¡ nuevamente al modelo, pero siguiendo esta metodologÃ­a. ProbÃ¡ hacerlo varias veces, con diferentes valores de `random_state`, o directamente sin especificar ningÃºn valor (para que `train_test_split` arroje siempre divisiones al azar diferentes)
>
> Â¿Ves ahora algÃºn cambio en los coeficientes y en <code>R<sup>2</sup></code>?
